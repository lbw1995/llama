{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ba368b-07b7-4967-81c9-6d5770340afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b6440-67c3-4235-ab38-22e4d5196020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRMSNorm(nn.Module):\n",
    "    def __init__(self,eps,dim):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.w = nn.Parameter(nn.ones(dim))\n",
    "    def forward(self,x):\n",
    "        y = x/(x.pow(2).mean(-1, keepdim = True)+self.eps).sqrt()\n",
    "        return y*self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d33a9-ca1b-4a24-a1bb-14551bc6d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.maximum(x, torch.tensor(0, dtype=x.type,device=x.device))\n",
    "    \n",
    "class mySiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)*x\n",
    "class mySwiGelu(nn.Module):\n",
    "    def __init__(self, inputdim, embeddingdim):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(inputdim,embeddingdim,bias=False)\n",
    "        self.w2 = nn.Linear(inputdim,embeddingdim,bias=False)\n",
    "        #self.w3 = nn.Linear(embeddingdim,inputdim,bias=False)\n",
    "    def forward(self, x):\n",
    "        #y = self.w3(nn.functional.gelu(self.w1(x))*self.w2(x))\n",
    "        y = nn.functional.gelu(self.w1(x))*self.w2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ab0bb3-1f43-4b01-a261-c994d20276ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5603, 0.6888, 1.4021, 0.7771, 1.1931, 0.2999, 0.8704, 0.2172, 1.4357,\n",
       "        1.5042])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([10])\n",
    "eps = 1e-6\n",
    "x/(x.pow(2).mean(-1, keepdim = True)+eps).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc45fa-c8f1-4e4a-802f-7cc6c6f3fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f5827-b925-47be-b555-4bb47e77c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myrnn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear(vocab_size, embedding_size)\n",
    "        self.ln2 = nn.Linear(embedding_size,embedding_size)\n",
    "        self.o1 = nn.Linear(embedding_size,vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x, state):\n",
    "        newstate = self.relu(self.ln1(x)+self.ln2(state))\n",
    "        return self.o1(newstate), newstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b30241-88b6-43f0-85be-30dd2b7e045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec71e93d-eadd-4007-aef8-375266478454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X,valid_lens,value = -1e6):\n",
    "    shape = X.shape\n",
    "    for i in range(shape[0]):\n",
    "        X[i][valid_lens[i]:] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd467fd-8330-44f6-bd5d-0759194b3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=-1e6):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfb655cf-072f-43b7-ae47-ea6e09348c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    #X bs*seq_len*embedding_size\n",
    "    #valid_len bs*valid_len or bs\n",
    "    #X bs*q_len*k_len\n",
    "    #valid_len bs*valid_len or bs\n",
    "    \n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        X = sequence_mask(X.reshape(-1,shape[-1]), valid_lens)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2940f64-790a-45ee-9aa1-77022e0efbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class additive_attention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
    "        super(additive_attention, self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias = False)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias = False)\n",
    "        self.W_v = nn.Linear(num_hiddens, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
    "        features = queries.unsqueeze(2)+keys.unsqueeze(1)\n",
    "        scores = self.W_v(features).squeeze(-1)\n",
    "        self.attention = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention), values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "187ca306-d717-4216-aefe-7b1b952fb931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries,keys.transpose(1,2))/math.sqrt(d)\n",
    "        self.attention = masked_softmax(scores,valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention), values)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac6b5d85-a9ab-4b5e-825c-90cc583f0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    X = X.reshape(X.shape[0],X.shape[1],num_heads, -1)\n",
    "    #print(X)\n",
    "    X = X.permute(0,2,1,3)\n",
    "    X = X.reshape(-1,X.shape[2],X.shape[3])\n",
    "    return X\n",
    "def transpose_output(X, num_heads):\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3fa6c5df-6a1d-4f07-aae4-2440b99b880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        #print(transpose_qkv(self.W_q(queries), self.num_heads))\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "        #print(queries)\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)\n",
    "        attention = self.attention(queries, keys, values, valid_lens)\n",
    "        output = transpose_output(attention, self.num_heads)\n",
    "        return self.W_o(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53b3d8-078c-4696-a503-559085de1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "class dotproductattention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "    def forward(self, quiry, key, value, mask):\n",
    "        d = quiry.shape[-1]\n",
    "        score = torch.matmul(quiry, key.transpose(-2,-1))/math.sqrt(d)\n",
    "        if mask:\n",
    "            score = score+mask\n",
    "        return torch.matmul(self.dropout(F.softmax(score, dim=-1)),value)\n",
    "class multiheadattention(nn.Module):\n",
    "    def __init__(self, nquiry,nkey,nvalue,embedding_size,nheads,dropout,bias=False):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(nquiry,embedding_size,bias=bias)\n",
    "        self.wk = nn.Linear(nkey,embedding_size,bias=bias)\n",
    "        self.wv = nn.Linear(nvalue,embedding_size,bias=bias)\n",
    "        self.wo = nn.Linear(embedding_size,embedding_size,bias=bias)\n",
    "        self.attention = dotproductattention(dropout)\n",
    "        self.nhead = nheads\n",
    "    def forward(self, quirys, keys, values, mask):\n",
    "        quirys, keys, values = self.wq(quirys), self.wk(keys), self.wv(values)\n",
    "        quirys = quirys.view(quirys.shape[0],quirys.shape[1],self.nheads,-1)\n",
    "        keys = keys.view(keys.shape[0],keys.shape[1],self.nheads,-1)\n",
    "        values = values.view(values.shape[0],values.shape[1],self.nheads,-1)\n",
    "        score = self.attention(quirys,keys,values,mask)\n",
    "        score = score.view(score.shape[0],score.shape[1],-1)\n",
    "        return self.wo(score)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99923a-134a-44d6-b07e-a47c64608b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionencoding(nn.Module):\n",
    "    def __init__(self,seq_len, numhiddens, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        x = torch.outer(torch.arange(seqlen,dtype=torch.float32),1.0/torch.pow(10000.0, torch.arange(0,numhiddens,2.dtype=torch.float32)/numhiddens))\n",
    "        self.P = torch.torch.zeros((1, max_len, num_hiddens))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6c8627c-b56d-47fe-a80a-6fd4a33a7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d2a8d9e0-f5bd-45ac-9ff1-fe3e2a581a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens,ffn_num_outputs)\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb9975f3-a35f-4dac-afce-d07cb263bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y)+X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bec490ca-a07a-4230-8db8-4a40a11cb242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **args):\n",
    "        super(EncoderBlock,self).__init__(**args)\n",
    "        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input,ffn_num_hiddens,ffn_num_input)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y=self.addnorm1(X, self.attention(X,X,X,valid_lens))\n",
    "        return self.addnorm2(Y,self.ffn(Y))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9bb1faa9-3576-48db-8488-7a7bf23627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **args):\n",
    "        super(TransformerEncoder,self).__init__(**args)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),EncoderBlock(key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens,num_heads, dropout, use_bias))\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i,blk in enumerate(self.blks):\n",
    "            X = blk(X,valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "967cf10a-2506-4eaf-9457-7528d908a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, use_bias=False, **args):\n",
    "        super(DecoderBlock, self).__init__(**args)\n",
    "        self.i = i\n",
    "        #层数\n",
    "        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input,ffn_num_hiddens,ffn_num_input)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None   \n",
    "        X2 = self.attention1(X, key_values, key_values,dec_valid_lens)\n",
    "        Y = self.addnorm1(X,X2)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs)\n",
    "        Z = self.addnorm2(Y,Z)\n",
    "        return self.addnorm3(Z,self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bea82e5e-b988-4c89-a99f-9ced7656de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def _norm(self, x):\n",
    "        return x*torch.rsqrt(x.pow(2).mean(-1,keepdim=True)+self.eps)\n",
    "    def forward(self,x):\n",
    "        output=self._norm(x.float()).type_as(x)\n",
    "        return output*self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dbf36b42-c78d-4d80-ba7a-57b9863ac5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freq_cis(dim, end, theta = 10000.0):\n",
    "    freq = 1.0/(theta**(torch.arange(0,dim,2,dtype = torch.float)/dim))\n",
    "    t = torch.arange(end, device = freq.device)\n",
    "    freqs = torch.outer(t,freq).float()\n",
    "    freps_cis = torch.polar(torch.ones_like(freqs),freqs)\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "28e589e0-fd6e-4a0f-abd7-8230836ae4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freq_cis, x):\n",
    "    return freq_cis.view(1,freq_cis.shape[0],1,freq_cis.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cff43c28-e13e-46c8-b769-101c690e3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(xq, xk, freq_cis):\n",
    "    xq_ = torch.view_as_complex(xq.view(*xq.shape[:-1],-1,2))\n",
    "    xk_ = torch.view_as_complex(xk.view(*xk.shape[:-1],-1,2))\n",
    "    freq_cis = reshape_for_broadcast(freq_cis)\n",
    "    xq_out = torch.view_as_real(xq_*freq_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_*freq_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "517ddb7d-3e1e-4eae-bd08-b09e7337b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init___()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        model_parallel_size = fs_init.get_model_parallel_world_size()\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.wq = ColumnParallelLinear(args.dim, args.n_heads*args.head_dim,bias=False, gather_output = False,init_method=lambda x: x)\n",
    "        #why no xavier? torch.empty()????/\n",
    "        self.wk = ColumnParallelLinear(args.dim, args.n_kv_heads *args.head_dim, bias=False, gather_output = False,init_method=lambda x: x)\n",
    "        self.wv = ColumnParallelLinear(args.dim, args.n_kv_heads *args.head_dim, bias=False, gather_output = False,init_method=lambda x: x)\n",
    "        self.wo = ColumnParallelLinear(args.n_heads*args.head_dim, args.dim, bias=False, gather_output = False,init_method=lambda x: x)\n",
    "        self.cache_k = torch.zeros(args.max_batch_size, args.max_seq_lens, args.n_local_kv_heads, args.head_dim).cuda()\n",
    "        self.cache_v = torch.zeros(args.max_batch_size, args.max_seq_lens, args.n_local_kv_heads, args.head_dim).cuda()\n",
    "    def forward(self,x,start_pos,freq_cis,mask):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(keys, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(values, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43977198-6a78-43cb-bfcd-588ed547545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambd):\n",
    "        \"\"\"\n",
    "        前向传播：不做任何改变，直接返回输入。\n",
    "        ctx 用于保存lambda参数，以便反向传播时使用。\n",
    "        \"\"\"\n",
    "        ctx.lambd = lambd\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        反向传播：将梯度乘以负的lambda，反转梯度方向。\n",
    "        \"\"\"\n",
    "        grad_input = grad_output.neg() * ctx.lambd\n",
    "        return grad_input, None  # 返回对输入x的梯度，和对lambda的梯度（None）\n",
    "\n",
    "# 使用时定义一个函数来调用\n",
    "class GradientReversalLayer(torch.nn.Module):\n",
    "    def __init__(self, lambd=1.0):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambd)\n",
    "\n",
    "# 示例：如何使用 GRL\n",
    "grl = GradientReversalLayer(lambd=1.0)  # 设置lambda参数\n",
    "x = torch.randn(5, requires_grad=True)   # 输入张量\n",
    "output = grl(x)  # 前向传播：输出和输入相同\n",
    "\n",
    "# 模拟一个损失并进行反向传播\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca7df1-f12d-4ce3-9923-89627ee848e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad35ffb-096c-47c0-a05a-c1b142c0631a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4651c2a-6524-4ebe-8447-50a1b16e8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class BeamSearchNode:\n",
    "    def __init__(self, sequence, score, state):\n",
    "        self.sequence = sequence\n",
    "        self.score = score\n",
    "        self.state = state\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.score < other.score\n",
    "\n",
    "def beam_search(decoder, start_token, beam_width, max_len):\n",
    "    # 初始化beam中的节点\n",
    "    start_node = BeamSearchNode(sequence=[start_token], score=0, state=None)\n",
    "    beam = [start_node]\n",
    "\n",
    "    # 遍历每个时间步\n",
    "    for _ in range(max_len):\n",
    "        new_beam = []\n",
    "\n",
    "        # 对beam中的每个节点进行扩展\n",
    "        for node in beam:\n",
    "            # 使用解码器生成下一个时间步的候选项\n",
    "            candidates, next_state = decoder(node.sequence, node.state)\n",
    "            for candidate, score in candidates:\n",
    "                new_sequence = node.sequence + [candidate]\n",
    "                new_score = node.score + score  # 累加分数\n",
    "                new_node = BeamSearchNode(sequence=new_sequence, score=new_score, state=next_state)\n",
    "                new_beam.append(new_node)\n",
    "\n",
    "        # 选择得分最高的beam_width个节点继续搜索\n",
    "        beam = heapq.nlargest(beam_width, new_beam)\n",
    "\n",
    "    # 返回得分最高的序列\n",
    "    best_node = max(beam, key=lambda x: x.score)\n",
    "    return best_node.sequence\n",
    "\n",
    "# 示例解码器函数\n",
    "def example_decoder(sequence, state):\n",
    "    # 这里我们使用一个简单的示例，实际情况中应使用实际的模型和解码逻辑\n",
    "    next_candidates = [(0, -1.0), (1, -2.0), (2, -0.5)]  # 示例候选项和得分\n",
    "    return next_candidates, state\n",
    "\n",
    "# 使用Beam Search进行搜索\n",
    "start_token = 0\n",
    "beam_width = 3\n",
    "max_len = 5\n",
    "result = beam_search(example_decoder, start_token, beam_width, max_len)\n",
    "\n",
    "print(\"Best sequence:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b6f5ef96-8eae-4406-8d58-d3f9c98e8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):\n",
    "    pred_tokens, label_tokens = pred_seq.split(), label_seq.split()\n",
    "    len_predict, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0,1-len_label/len_predict))\n",
    "    for n in range(1,k+1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label-n+1):\n",
    "            label_subs[' '.join(label_tokens[i:i+n])]+=1\n",
    "        for i in range(len_predict-n+1):\n",
    "            seq = ' '.join(pred_tokens[i:i+n])\n",
    "            if seq in label_subs and label_subs[seq]>0:\n",
    "                num_matches+=1\n",
    "                label_subs[seq]-=1\n",
    "        score*=math.pow(num_matches/(len_predict-n+1),math.pow(0.5,n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6f61b511-c59a-4434-9989-9817cd12e1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.arange(1, 10).repeat(2,1), repeats = 2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8e9582f3-78bf-4d5f-9fed-400cf24fbe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "031cd81f-1e53-4efe-b05e-73e1017df3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm([3,4], 0.5)\n",
    "add_norm.eval()\n",
    "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1acf35-0038-4b30-b2e4-0f8cbb1d210c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e3a9887f-165f-43a1-8aeb-36a2e35bc8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0948,  0.0187,  0.4682, -0.3720, -0.7445,  0.1480,  0.8489,  0.5328],\n",
       "        [-0.0948,  0.0187,  0.4682, -0.3720, -0.7445,  0.1480,  0.8489,  0.5328],\n",
       "        [-0.0948,  0.0187,  0.4682, -0.3720, -0.7445,  0.1480,  0.8489,  0.5328]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 4, 8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aa69e762-fd5d-497e-9625-1d10ec8fab95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (Dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc0bb5dd-d1d1-473b-b453-2afb6249b436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens = 6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attention(X, Y, Y, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dbb4d02-66f7-4489-86d6-798dad12b0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [2, 3],\n",
       "        [2, 3],\n",
       "        [2, 3],\n",
       "        [1, 4],\n",
       "        [1, 4],\n",
       "        [1, 4],\n",
       "        [1, 4]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_lens = torch.tensor([[2,3],[1,4]])\n",
    "torch.repeat_interleave(valid_lens, repeats = 4, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "312f6e00-7482-48eb-8d3a-d2cc853a9114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = torch.normal(0, 1, (2, 1, 2))\n",
    "attention = DotProductAttention(dropout=0.5)\n",
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf395e1-088f-426f-8148-ab503f3879ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "613956bf-f24d-4bbf-b01f-6f0883afcb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
    " # values的小批量，两个值矩阵是相同的\n",
    "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n",
    " 2, 1, 1)\n",
    "valid_lens = torch.tensor([2, 6])\n",
    "attention = additive_attention(key_size=2, query_size=20, num_hiddens=8,\n",
    "dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38e76c32-9439-43b5-9dd2-1e0aa724567f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.eval()\n",
    "attention(queries, keys, values, valid_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e72441-9502-47f4-9388-4dd849a8e264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
